 Thanks. Recording in progress. Okay, great. So here's the outline of the talk today. So I'm going to start with motivating this paper. So there is a growing literature that shows that discrimination can be driven by biased beliefs. So traditionally in economics, we always assume that discrimination that are driven by beliefs are driven by accurate beliefs, and that's statistical discrimination. But recently people have shown that these beliefs can be wrong. So this presents a challenge because there is very little evidence on where these biased beliefs are coming from that's driving discrimination. And also we don't know how we can address it. There are many studies that have attempted to provide information to reduce discrimination. However, in many of these studies, information usually does not work in reducing prejudice and discrimination. And there is also a very large literature on providing information interventions in an attempt to change behavior. And in this broader literature on information interventions, there's also very mixed evidence on the effectiveness of providing information in general. So the way that we usually provide information in experimental economics is to provide explicit descriptions about outcomes and probabilities. However, in real life, uncertain situations very rarely come with descriptions that are neatly packaged. Except maybe weather forecasts. And when we make real life decisions, we usually have to think back and draw on our past experiences. And that's how we form beliefs and make decisions. And we also know from literature across many different fields that experience matters. So in education, we know that experiential learning improves learning outcomes compared to passively receiving information based on lectures. And in psychology and economics, we know that personal experience with minority groups can reduce prejudice. And in risk preference literature, we also know that learning from description and learning from experience can lead to diverging risk preference patterns. So we know that experience matters and different ways of providing information probably will lead to different outcomes. However, there is very limited evidence on exactly what the impact of information environment, which is how you provide information, leads to different outcomes. So here are my research questions. First, I'm interested in how information environment affects belief formation and also subsequent discrimination. So here I'm defining the information environment as the way that information is acquired or presented as opposed to what information is presented. And in particular, in this study, I'm going to be focused on information in the form of explicit descriptions versus through direct experience. And the second question is, if information environment does indeed affect beliefs and discrimination, can we find an information environment that can meaningfully reduce belief bias and discriminatory behavior? And in particular, I'm interested in whether learning from experience can lead to a reduction in belief bias and discrimination. Can I ask a question? Yeah. Do these two usually move together? Like if the belief bias is reduced, do people end up engaging in less discriminatory behavior? That's a good question. Or what they separate it? That's a good question. So the evidence is very mixed. So like I referenced here, a lot of studies that show attempt to provide information to reduce discrimination, they find a first stage effect on beliefs, but they don't find anything on behavior and discrimination. So that's what I'm trying to find out because these studies all use explicit descriptions. So my hypothesis is by providing direct experience, people can lead to meaningful changes in behavior as well. So why do we want to study description and experience in particular? So as we mentioned before, real life experience, real life decision making often needs to rely on experiences and we don't have access to description over probabilities and outcomes. And also there's recent evidence that shows that learning about uncertainty from description versus from experience may correspond to different cognitive mechanisms. And additionally, I also want to point out that the type of experience can also matter because we know from existing studies that when people seek out information endogenously, that can actually reinforce bias. So here I'm going to study two types of experience. One is endogenous experience. Another type is exogenous experience. So how I operationalize the particular information environment in this paper is first, the description I provide is a histogram of group level distribution of productivity. And experience is by sampling individuals from the group. And voluntary experience is endogenous sampling, which is voluntary sampling. They can sample as many as they want. And fourth experience is exogenous sampling, which is you must sample an exogenously assigned number of people. So the contributions of this paper, first, I document a novel source of bias beliefs, which is information environment. And my work is very closely related to the literature on belief driven discrimination. And I also propose an effective behavioral intervention to reduce discrimination and correct bias beliefs. And this contributes to the literature on information interventions in general. I also provide evidence on a new mechanism of belief formation and extend the literature on the description experience gap by showing that providing description versus experience leads to different patterns of beliefs and discriminatory behavior. So now I'm going to move on to experimental design. So the key design elements for the employer experiment, the structure of the experiment is first, I'm going to elicit group level beliefs about worker productivity. So each employer has a worker group. And after I elicit prior beliefs about group level productivity, the employers will receive some information about the productivity distribution in the group. And this is where the treatment manipulation happens. So I'm going to present information in different ways in the form of description versus experience. And after they receive information, I'm going to elicit updated group level beliefs. And finally, after updated beliefs, the employers are going to make six wage offers to six randomly selected workers from their group. So the treatment design is a completely between-subject design. There are five treatment arms. And within each treatment arm, there are two race treatments. So let's talk about the information conditions first. So in the description condition, I provide a histogram of the distribution of worker productivity to the employers. And in the voluntary experience condition, the employers can voluntarily sample workers from their worker group. And there are two forced experience treatments. The first one is forced experience 11, which is I force the employers to sample 11 workers. And why 11? It's because the median number of sampled workers in the voluntary experience treatment is 11. So here, I remove the endogeneity in the voluntary sampling treatment and use the forced experience 11 treatment as an exogenous benchmark for voluntary experience. And the fourth treatment arm is forced experience 20, in which the employer has to sample 20 workers. The idea here is I want to increase sample size and reduce sampling error to see if that improves outcomes. And finally, there is a baseline, which serves as control. And in this baseline treatment, there is no information that's received by the employers. So they go from directly, they go from reporting the prior beliefs directly to the wage offers. I think Armand has a question. Yes, Armand. Armand, can you explain, like, when you say experience, is this, like, I'm drawing a number out of a hat? Or is it I'm actually, you know, the, so I don't know what this task is that people are doing, but, like, I actually have to sit in the logs that do the task and I see how many they get out of it? Because if it's sort of just, like, drawing out of a hat a number rather than seeing the whole Instagram at once, I feel like that's a little different than experience. Because that's, it's more about how the information is presented versus if it's, like, I actually observe the person doing the thing and get some real-world payoff or not based on that experience. Does that make sense? Yeah. Yeah, yeah, yeah. That's a very good question. Thank you. So, of course, it is about how information is presented. So, the primary variation in this treatment, I mean, in this study is to vary how information is presented and see if we can change outcomes. So, that is one part of the answer. But, also, I will talk about why exactly I'm calling the experience in a bit. So, but to basically, like, provide a brief answer, I think the reason I'm calling experience is twofold. So, first, I'm basically adopting the convention in the risk literature because they, in the literature on the description experience gap, experience basically refers to sequentially sampling the lotteries. So, I'm adopting this convention of naming. And, secondly, in psychology, experience is defined as an event that is lived through. So, you go through something and you observe the outcome. And, here, you are clicking a button and sampling a worker and then you observe the outcome of the worker. So, that's to mimic the experience part. But, obviously, it's very different from observing real-life people doing something. But, this is basically what we're trying to proxy in the lab environment, in the control environment. That makes sense. Yeah. One follow-up question is, maybe I'll get to this too, but assuming that the histogram is over 20 workers, and so the force experience 20 is the entire histogram, I'm assuming you get exactly the same information, so it perfectly matches. So, maybe that one versus the histogram makes sense. Maybe 11, in a sense, you could get a good or a bad draw. And so, it could, or is part of the question, like, how well do people understand that, do they also see the histogram? No. So, yeah, in the experience treatments, they don't see the histogram. So, then how do we control for, like, the, how do you put it, the... Information content? The natural variance, the natural variance that they'll be with lower draw than the higher draw. So, they might get a more biased sample. Yeah, exactly. So, that's actually part of the design feature. So, because the force experience 11 is intended to serve as a benchmark to voluntary experience. So, because both of these treatments have the same median number of draws, so the average information content is held the same, and the time they spend on the drawing is also the same. So, by doing the experience 11 treatment, I'm trying to remove endogeneity, information seeking, and see if the endogeneity matters. And, why I do the force experience 20 is because I want to exactly reduce the variance in the sampling, and also increase the sample size. So, this allows me to reduce the sampling error, and see if that improves outcome compared to force 11 treatment. Does that make sense? Yeah, yeah, that makes sense. Thank you. Okay, thank you. Christophe? Christophe? Is a potential answer to some of Armand's concerns that, like, your results are going to be that this voluntary experience does better than the histogram? So, like, if we'd thought, you know, seeing people do the thing in reality, that's, like, more experience than just drawing a number out of a hat, but even drawing a number out of a hat is enough to... Yeah, that's a good point. Thank you. So, the question is, um, it's part of the answer that, um, we observed that, uh, learning, drawing from, uh, drawing workers from the group in this force experience treatment, um, already achieves, uh, better outcomes. So, we don't need to actually, um, go into the real world and observe workers. Or, like, that this provides a more efficient, uh, intervention that we can employ. And the answer is yes. Uh, so, because even though this is a preview of the results, even though in the description, the histogram theoretically provides the most amount of information, what I observed in the results is that in the force experience treatments, beliefs are able to completely converge and, uh, wave discrimination is completely eliminated. So, that basically says that we actually don't need to have a very complicated mechanism, uh, to force people to go through, you know, like, watching people perform tasks or whatever. And, uh, like, in actuality, this very simple mechanism of sequential sampling and observing the outcomes for each worker is sufficient to, uh, achieve the results we want. Um, but that will come up later. Is it possible? One follow-up on this, super interesting, um, thanks Christoph, that was, that's a clarified note. And then it makes the question, you know, the endogenous, if 11 is the number they're endogenously selecting out of a, a total sample of 20, and I'm assuming it's without replacement, um, like, would it be an endogenous amount, but just given the size of the distribution? Sorry, I don't. Or something like that, because 11 out of 20 might give you a pretty good, uh, draw on the mean. But if you have some, I guess, it's not about the number of 20, but it's about 11, like, it, you know, you can get bad draws and end up with, uh, more discrimination than if you don't sample enough. And so, sort of like, what's driving people to choose 11, which maybe is, you know, sufficient in this context to, to get the true mean, versus why some people might do less synonymously, and then so on and so forth. I think that becomes really important then, because there are going to be the corner cases where this would make it worse. So you're talking about, you're talking about the voluntary sampling, right? Yeah, like, if you only sample two people and then you draw a complete conclusion from that, you're probably going to get wrong a lot of the time. Yeah, that's a good point. So I do talk about that later. Uh, so, uh, can we, uh, can we move on and then maybe I will illustrate this point later? Yeah, sorry. Thanks. Thank you. Yeah, appreciate it. Okay, so, that's the information conditions plus the baseline without information. And within each information condition, each employer is randomly assigned either an Asian worker group or a Hispanic worker group. Um, and the, sorry, overall, there's only one Asian group and one Hispanic group, and they're randomly assigned to each worker, I mean, each employer. So the advantage of this completely between design is one, we can remove order effects, and two, it helps to mask the true purpose of the experiment. Uh, and that allows us to reduce experiment demand. Sorry, can I just, one follow up on the information. Yeah. I mean, is it possible that, like, the reason why the voluntary or the one at a time is better, right, in terms of reducing just one, is because you get time to process it? Like, if I just see it, like, that's, so it's not, yeah, so. Exactly, I will actually talk about that right now. Yeah, go ahead. Thank you. I just didn't understand what that said. Oh. What is? Oh, so it's a five by two between subject design. These are the five information conditions. And then within each information condition, you get either an Asian or Hispanic worker. What is a between subject design? For people who don't know between subject design. Oh. Uh, between subject design is, uh, randomization is happening at the subject level, so you're randomizing to either this condition or that. Uh. Randomly? Maybe you could tell me, like, what an order effect is? Oh, so basically, um, in, uh, in experiments, uh, if we present both Asian and Hispanic workers, necessarily I'm gonna have to ask you one and then the other, and then that's order effect, because which one you see first might influence how you answer the other. Randomly, it's like one, I see, so they're never gonna be asked. They're never gonna be seen two groups. They're only gonna see one group. Yeah. Okay. So, yeah, if I employ, if I'm an employer, I have an Asian group, or I have a Hispanic group. Never gonna be both. Yes. Yeah. If there's like other dimensions that we think people, like if I think this is a predominantly male group versus a predominantly female group or something, like, are they told things of like the gender representativeness of the group, like it's representative of the population, nothing about the gender, like, because I could think I might have other, like I might correlate some of these things with other things over which I would have. So, I will talk about that later. Okay, so, so next I'm gonna talk about the worker part of the experiment. So, the dimension of the... Can I, yeah? But isn't the answer to that question that you are looking at treatment differences? So, even if one of the races triggers a particular gender in your head, that shouldn't be driving treatment differences. Right, yes. And also I do control for that. So, that, I'm gonna talk about that later. Um, so, the dimension of discrimination is ethnic discrimination. So, I'm looking at Asian versus Hispanic, uh, workers. The workers take a math test of 12 questions, and they got a score which equals the number of correct answers. And that is a proxy for their productivity. Um, and the reason I use math is because Asian are stereotypically, uh, good at math. So, that produces initial belief variation. Uh, the worker groups, again, Asian were... Say the whole sentence. And Hispanica, Hispanica... Well, uh, the predominant stereotype of Hispanics is that... Lays. They might be less good at math. Uh, huh. But, again, that's why it's a stereotype, because, you know, it's not true. So, there are a hundred workers in each worker group, and I construct a group so that they have the same score distribution. So, same mean, same distribution. And there are no interactions of the workers with the employers, so it's a static game. I mean, it's a static environment, no games. And the payoffs of the workers are not affected by the employer decisions, and the employers know this. So, that gets rid of interpersonal preferences. So, employers are different subjects from workers. Again, the order of the tasks, as we mentioned before. First, reporting initial group-level beliefs. Two, information... Receive information about the score distribution in the worker group. Three, report updated group-level beliefs. And then, four, make six wage offers to six randomly selected workers. And the six... Uh, the wage offer is basically a willingness to pay for the worker. And the mechanism I use to elicit this is a multiple price list. And I incentivize it using a BDM mechanism, which is incentive-compatible. Okay. So, I'm going to walk you through the tasks in each of the conditions. Um, I'm going to start with prior belief elicitation. And this part is identical in all of the treatments. So, um, the advantage of that is it allows us to generate comparable prior beliefs in all of the treatments. So, in the very start of the experiment, I informed the employers about their experience. They're assigned a worker group, either Hispanic or Asian, and the worker score calculation. And the prior beliefs I elicit... Excuse me. Um, I have a bad lingering cough from being sick. So, the prior beliefs I elicit is at the group level. And, there are three things I elicit. One is the subjective belief distribution of the scores in the worker group. per group. So it's basically a probability mass function. And the advantage of this is by getting the entire belief distribution, I can disentangle the source of the discrimination as basically whether it's belief or it's taste. And second is I directly elicit the belief on the group average score. And the reason I also do this is because it's a much more intuitive measure. And this is also going to be the main belief outcome I focus on for this talk. I also elicit belief on the average age of the workers in the group. This is just a filler task to basically help reduce experiment demand and mask the true purpose of the experiment. And again, prior belief tasks are identical across all measurements. OK, so after reporting initial group level beliefs, the employers receive the information about the score distribution. So in the description condition, this is what they see. So they see a binned Instagram distribution about the scores in the group. So there are four bins, 1 to 3, 4 to 6, 7 to 9, 10 to 12. And I do tell them there are no one that got 0. And I give them- for both groups. Yes. These are the- Yes. They only- This graph is the same for both groups. So I give them ample instructions on how to read a histogram. And this is the information in the description condition. In the experience voluntary condition, they're going to be voluntarily sampling. So how do I make an experience? So the key thing here is, one, they can only draw one worker at once. And there's a three-second cool down on the button that they click to draw the worker. And after they draw the worker, they see the outcome of this worker in the bin format. This is to make sure that the information between the different treatments are comparable. And when they click it again, the second worker's outcome is appended at the bin. So basically, this makes sure that you're going through one work at a time, and you observe the outcome, and then you go through another worker, and then you observe the outcome for the other worker. And this drawing mechanism, this button, is the same for all the experience conditions. Here, in the voluntary, since it's a voluntary sampling, they can do as many times as they want. It's random sampling with replacement. And here, obviously, we have endogenous information acquisition. So essentially, we have an optional stopping problem. So they can decide when to stop sampling. And the sample size is going to be endogenous, and also can be endogenous to the sampling distribution they observe. So suppose I press the draw worker button, and I get a worker score between 4 and 6, and then I press again, and it doesn't change. Now, is it because the computer didn't work, or I draw a worker with the same score? No, you get the next one below. Do I have some feedback on this? You get the next one below, so it's appended. Ah, OK. Yeah, yeah. OK, good. I think Armand, did you have a question? Yeah, my question was, at this point, what is there? How did you frame their incentive to collect it? Like, are they told now, after you collect information, you're going to be asked to give this distribution again, and pay based on how correct you are? Yeah. Or they know they're going to be giving a contract, and that their wage payout will depend on something about... Ah, yes. Good question. So, short answer is, they're all incentivized. But I will talk about the information incentives in detail after I go through the tasks. Is that OK? But they know, the question is where they know the incentives as well. They do. They do. Yes. Another question, sir? Yeah. So, what is the advantage of having this person click 10 times, instead of picking a sample size, and then having the distribution show up in one? Yeah. So, that's a good point. So, in this treatment, I want to have the information-seeking behavior be entirely endogenous. Because here, the sample size is not only depending on what kind of, like how large a sample you want, it can also depend on the sampling distribution. Because you are observing the sampling distribution as you decide whether to stop. So, that's an additional layer of endogenousity that I want to preserve. I think, I think, deciding the sampling size first, and then drawing the sample, could be a very interesting treatment to disentangle some mechanisms. Yeah, I would just add to that, any small friction you have in the task is going to make it more salient as an experiment. Exactly. Can I add a couple of things to that? Yeah. So, it's also about, like, endogenous information acquisition is also about, you might try to capture a person who really wants to see bad information about Hispanics. So, if they see two bad signals, they're already happy and they're stuck. Exactly. And if they see two good signals, then they are like, okay, let me just try again. I'm sure, you know, bad news is coming. And that helps you kind of get to that. Exactly. So, that, yeah, exactly. I would call it like a realization-based sampling, right? Like, you see the realization and then you sample it again. Yeah, that's, yeah, that's a good point. So, I guess that wasn't really clear. That's what I meant by the sampling, the sample size being dependent on the sampling distribution. Like, you can decide to stop when you see the information you like, vice versa. I wouldn't say that it depends on the distribution because it's known. It depends on the realization. Right, okay, yeah. Good point. But the distribution is not known. In this one, it's not true. That's right. It's not known. Didn't they see the thing? No. This was the other treatment. Ah, sorry. Okay. So, in the force experience treatments, this is the force experience 11. Uh, Chris? Like, hear what you all are saying about, like, that's really interesting. Plastic here? Yeah. Like, think about it. But, like, you know, I feel like the downside is, I don't, like, what are they supposed to do? Like, what do, would it, would it, like, I'd like to understand, like, if, if they're a rational Bayesian and, you know, they're allowed to choose their sampling strategy, I don't know, they have some, like, some time cost or something, I don't know, maybe they should, like, I guess it depends on what you, what you want to conclude from this, like, so, yeah, I don't know, maybe they should, like, I guess it depends on what you, what you want to conclude from this, like. So, if I understand correctly, the question is, what is the optimal strategy for rational Bayesian? That's a good question. So, I think the optimal strategy for rational Bayesian would be to, one, determine your sample size, uh, x-ante, so that, you know, it's not dependent, that your sample size shouldn't be dependent on the realization. And, two, you should basically, uh, have a large enough sample, or sample until the point where the marginal cost is the same, uh, equal to the marginal benefit, in terms of, you know, like, earnings based on accuracy. It's not very, it's like a bandit problem. You stop sampling at some point. It's. But, but, but, you have kind of, like, a cutoff, right? Well, it's not exactly, it's. But it's not determined, it's under the sampling size, it's not the expected sample. The expected sample size would be the term, that's right, but not the, but not the actual sample. Right, but also it's not exactly the same as a bandit problem, because here there's no exploration going out, because you already know the outcome space. No, you don't know the distribution, you're trying to learn about the distribution. Right, you don't know the distribution. So, so, so, this is a bandit problem. But it's a one arm. Yes, it's a one arm bandit problem, and then outside optionally when you stop, you. Right, okay, that's a good point. So, if they were forced to stay there for, I don't know, five minutes in this, regardless of how much they sample, the optimal thing to do would be to sample all, as many times as you can, right? Yeah. Because the only cost is waiting. Yeah. And if attention is costly. Like if they have to pay attention again, maybe that's costly, right? Okay. Because some cost in time, cost for time. No, but that's what I'm saying. If they were supposed to stay there for five minutes, either doing nothing or sampling. If you fix the time. If you fix the time, the optimal thing to do would be to sample all the time, and let them get a headache. For sure. Or if they can just be on their mobile, right? Or, yeah. Fix me for five minutes, I'll just be on my mobile for five minutes. Right. You fix the attention to the problem. Yeah. Exactly. Glue their head to the . So, like, I kind of just want to say something that Jacuma said. Like, maybe, you know, like, if one thing that you wanted is this, like, fraction of, like, having to click, you could implement Jacuma's idea by saying, like, please tell me a number of times that you would like to be forced to click, right? And then they say 11, and then they still have to do, they have to click draw worker 11 times. Right. So, actually, to your point, the idea of the click is actually not to implement frictions. The idea is to make it resemble an experience. So, because I want the sequential observe, sequentially making an action and then observing an outcome. That's why we're doing this click. So, I do not want any additional friction to remove your endogeneity. So, yeah. So, like, I'm going to go through the forced experience condition. Can I answer? Yeah. Are you saying that, like, this somehow resembles real life? That you know, I mean... Like, that, you know, you can't just decide, oh, I'm going to meet, like, 20 Hispanics today. And it's rather easier that, I'm going to meet a Hispanic guy, maybe he'll introduce me to another friend. I can say yes or no at that point. So, going one by one is more realistic then. Is that part of it? I mean, it is part of it, but obviously I cannot claim to that this completely resembles real life. So, I mean, like, because in real life, obviously, yes, when I'm meeting people and learning about them, I usually have to do it one by one. And that's a more naturalistic setting to do sequential sampling. And that's why I'm trying to proxy this by doing this in the lab. But, again, like, it's obviously different. But this is the best we can do in a controlled setting. Okay. So, in the forced experience condition, same button, they have to sample 11 workers. In the forced 20, they have to sample 20 workers. And after they get the information, they report updated group level beliefs. And these are the same tasks as the prior beliefs. So, you have the belief distribution, average score, and average age. And finally, in the baseline condition, they don't receive any information. And this is basically a control condition. And after reporting prior beliefs, they go directly to wage offers. And the wage offers happens after the FBD group level beliefs. And basically, they're going to observe sequentially six randomly selected workers from their worker group. And they're going to report the willingness to pay for each worker. They know that the wage offers do not affect the worker payoff. So that gets rid of the interpersonal preference problems. And, again, the wage offer block is identical for all treatments. So, how this is done is each worker has a profile. And this is an example Asian worker profile. So, the ethnicity of the worker is conveyed by the avatar and the name, the nickname. They know it's an anonymized nickname. And the idea is here. Doing this reduces saliency of ethnicity and hopefully reduces the experimenter demand. And also, I provide some additional information that are potentially irrelevant. The ethnicity is conveyed only by the nickname and the avatar? Didn't they know which group they were selling? They do. They do. But when they're making wage offers, I don't mention the Hispanic or Asian word. Ah. Yeah. So, maybe we just call this example worker profile. It is an example worker profile. Worker, not Asian. Not Asian worker. Oh, okay. Yeah. Okay. Example worker profile. Good. Okay. So, and I provide a variety of information that are potentially irrelevant because this represents a more naturalistic setting in which employers evaluate workers based on, and they can observe some information. And I throw these into the regression lab controls. Here's an Hispanic worker with an avatar. Is country always US? Yeah. They're all from the US. So, this is the interface for eliciting the willingness to pay. So, it's a standard multiple price list mechanism. There are 12 rows. In each row, you face a choice between two decisions. On the left is to hire the worker and earn the worker's scoring dollars. And on the right is to not hire and earn a fixed amount. And the fixed amount goes from $1 to $2 up until $12. And the incentive mechanism is the BDM. So, basically, at the end of the experiment, one of the rows is randomly selected. And whatever the selected row is, your choice in that row is implemented. So, for example, if this work row is selected to be paid a bonus here, then my choice is to hire the worker. Then my bonus is whatever this worker's score is in dollars. And basically, the gist, the upshot of this is it's an incentive compatible mechanism. And your dominant strategy is to truthfully report your willingness to pay. And you should switch at your indifference point. And that is going to be our wage offer. So, what is the role of a wage offer? It doesn't affect the workers' payoff. No, it just... It doesn't affect the employers' payoff. Yeah, so it just... The reason why it doesn't affect the workers' payoff is I want to remove the interpersonal preferences because that can affect the willingness to pay if it affected workers' payoffs. Here is just a very controlled environment. You are stating what is your willingness to pay for workers. And arguably, this we expect a weaker effect of discrimination. And the fact that we still observe one can, you know, like that is more confidence that, you know, the design is indeed effective. And it's the wage offer here, it just serves as a... It affects employers' pay. The employers, yes. Not the workers' pay. So if you don't hire, you get the score. If you hire, you get... Oh, it does affect the employers' payoff, yes. Okay. Because it's incentivized. I get an wage. It's incentive compatible. So, how the incentive works is one question is randomly selected at the end of the experiment. This goes back to your question, Armand. If the selected question is belief, they're incentivized based on accuracy. They get $4 if their answer is within plus minus two of truth. And if the willingness to pay question is selected, one of the rows is randomly selected and their decision is implemented. That's the classic BDM value of the station mechanism. Okay. So, there are two incentives. There are two reasons why a subject would like to sample. One is to make a better hiring decision. Mm-hmm. And one is to give a more accurate answer to whatever question. Yes. Yes. Okay, so... Is that problematic that there are two reasons why... Could you hedge? Could you hedge? Yeah. Could you hedge? Is that a little bit low? Because by randomly selecting one to pay at the end of the experiment, it is incentive compatible. And it's shown by Healy and Erzeli paper. I forgot which year it is. Okay, so here is a flow chart of the entire experimental design. These are the four information conditions. The yellow boxes are the treatments. You have three task blocks, prior beliefs, information, updated beliefs, and wage offers. Okay, so implementation, pre-registered on as predicted, programmed in quadrics, ran on Prolific, and this design allows me to get a wealth of data on beliefs, wage offers, and also sampling behavior, which is really interesting to look at. Okay, so results... Let's start with prior beliefs. So here I'm presenting on the left a bar chart on the average belief for the Hispanic worker groups and the Asian worker groups. We see that there is indeed a significant belief bias and belief gap between the two groups. On the right is the... So for the... So the two groups were designed in a way that they have the same score. Yes. But suppose you wouldn't have designed it. What would have been the score? How close would it be? I... To this. Yeah, I... That's a good question, but I don't know off the top of my head. Sorry, so this is prior, before this... This is the prior, yes. And this is pulled across... But... I mean... The two groups are designed in such a way that they have the same distribution. But they don't know it. The subjects don't know it. No, but... But how... How close is it to... Oh, I see. I see. I see. To the real. Yeah, like what is... I don't know the real. Christina has the data. Yeah, I have the data, but I don't know. Right now. On the right is... Still the way around. On the right hand side... On the right hand side, I am presenting the cumulative distribution function, the empirical distribution of the beliefs for the Hispanic, which is the red line, and the beliefs for the Asian workers. So basically it confirms what we see on the left. The prior beliefs for the Asian workers first-order stochastic dominates, the beliefs for the Hispanic workers. And here I am presenting prior group-level belief distribution by treatment. This is basically to show that the distribution are very similar across treatments, so we can pull them in the priors. And the Kruskal-Wallace test, which is basically a multi-sample met with the EU test, is insignificant. Do you think about if the race of the employer matches the race of the employees, how that would change what the beliefs look like? Yeah. If the in-group versus out-group might accentuate or attenuate some of these biases? That's a very good question. Because that's quite a large gap, I might think. But if I'm a Hispanic employer, is that gap smaller? Is it what they look at? Yeah, that's a good question. I can look at that. I just don't have that yet. And also, in the data, the Hispanic and Asian employers are very small fractions. So we might have more noise. But again, I can look at it. OK. So next, I'm going to look at updated beliefs after they get information. So the main question we're interested in is, can this new way of providing information through experience reduce belief bias? So since we're interested in reducing discrimination, then our primary goal for changing the beliefs is to reduce belief bias. There's two ways we can look at bias. So the first one is directional error from the truth. So whether you're systematically underestimating or overestimating the productivity. And the second way is we can look at the belief gap between the Hispanic group and the Asian group. And again, in the truth, in the two groups, the truth is that the distribution and the mean are exactly the same. So that serves as an objective benchmark. So here, I am plotting the mean directional error in the updated group-level mean beliefs. So I'm also putting the prior beliefs here on this first set. And on the right-hand side are the four information conditions. So we can go through them one by one. So in the priors before learning, the left-hand side is the Hispanic and the right-hand side is Asian. So we can see that people are systematically underestimating Hispanic productivity and overestimating Asian productivity. In the description condition, after they see the histogram, we can see that Hispanic, which is the blue one, actually overshot. So they're now overestimating everyone. But the belief gap has shrank, although there is still a significant belief gap between the two groups. So we're not able to completely reduce the belief gap through description. So in the description treatment, they are given the distribution for the entire group. Yes. And yet, they disagree. Exactly. In the voluntary experience... They don't believe it. They don't believe it. Well, they do take it into account, so they change their beliefs. But it's not sufficient. In the voluntary experience, where they engage in voluntary sampling, the Hispanic beliefs are just overall pretty accurate. But they are still overestimating the Asian productivity. So what we have is actually quite a large gap between the two groups. Is there any difference in the number of voluntary sampling between the groups? That could explain this, right? There's only one voluntary sampling group. Between yellow and yellow. Some people see Asians, some people see Hispanics. Oh, okay, yeah. So, yes, people actually do sample more for Asians. I'm really drunk. Yeah. Oh, that's interesting. It is interesting. It's just I haven't been able to do a phone analysis on the data yet, so I'm not going to present that here. Okay, so let's look at the forced experience treatments. Now, the forced experience, the forced 11 and the forced 20 are very similar. And both of them have a little bit of overestimation for both groups. But the interesting thing is, in the forced experience, we're able to close the belief gap entirely. So this means that where description has failed, forced experience has won. So even though the forced experience has less information content, so that's pretty interesting. So did you see if many people replied to the mean belief with an integer number? Did people know that they could have said 7.75? Yeah, so they do. But also, obviously, the spikes are the integers. But there are people who say, you know, like 7.3 or whatever. So I was worried about many people saying 7 or 8. People saying 7.3 is perfectly fine. Yeah. So did many people say integer numbers? Yeah, many people did. But also, many people did say decimals. So? And Armand has a question on that. Oh, Armand? Thanks, John. For this overestimation, given for the description, one at least you're showing them, actually for all of them, you're showing bins. Yes. But this overestimation, is it within the bounds if they just took the highest point of every bin? Within the bounds? Oh, I see, I see. But if they assumed people were always the highest point of a bin, then their mean is a point here, but would it be within the truth then? Yes. Does that make sense? Yeah, good point. So they are all within the bounds, but the bounds are quite wide, so it's not super informative. Well, that means that it makes it a little tricky to say that their beliefs are wrong, the mean belief is wrong, because, or overestimation, because they are within the bounds of what you gave them. So it's just, it's not the true mean, but it's, I wouldn't call it overestimation. I think a different word is like they're more optimistic within the bounds. I don't know how to think about it, but yeah, for how to say it, but I wouldn't call it overestimation. That makes sense. That's an interesting point. Yeah, thank you. I will have to think about that. That's just for all of us, right? Like when, you know, for example, the first one under description, like you're saying we have some difference between vision and Hispanic, but that could be explained by them putting the Spanish at the bottom of the- Oh, yes. Yeah, that's a very good point. So Christoph's point is, like it can, the belief variation can be explained by them shifting the distribution to the right or the left, right? Yeah. And that's a good point. I can actually check that in the data by looking at their belief distribution and I do see that pattern. I'm just not going to talk about it here. Yeah. That's a point of viewming. Yeah. Yeah. So they are, in fact, wiggling inside the bins. Okay. So here is the objective beliefs and the takeaway is basically, forced experience is able to completely reduce the belief gap. Here I'm also plotting the distribution of the posterior beliefs by treatment and it's just to show that, you know, what we see in the bar charts, I mean, the graph here, we also see in the distribution. The left is just the scale of the absolute, I mean like not absolute, but the scalar belief means. So this is a description, you can see that there's a belief gap. Voluntary, there's a significant belief gap. Force 11 converges. Force 20 converges. Okay. So taking stock in the belief bias. So we can see that description alone cannot eliminate belief bias, even though it has theoretically the highest amount of information. And we also see that the type of experience does indeed matter. So voluntary experience actually does not like closely, uh, uh, belief gap at all. So it actually reinforces existing bias. And by removing the endogeneity in information seeking, we can reduce the bias meaningfully. So, basically the gist is the force experience treatments, uh, exogenous sampling can eliminate the belief gap between the Asian and Hispanic groups. So, next question I want to ask is, does reducing belief bias meaningfully reduce discrimination? Because many studies on information provision have used descriptions and they did find a significant first stage effect on beliefs, but they found no effects on discriminatory behavior and attitudes. However, the majority or most of those studies are using explicit descriptions. So we have now seen that this new wave of, we have used a new wave of providing information, which is through sequential sampling or forced experience. Um, it has a significant first stage effect on beliefs. So then this can, this new way of providing information also reduce discrimination. So, how do we measure, uh, reduction in discrimination is by seeing whether we can close the wage gap between the Hispanic and the Asian workers. And here I'm going to present, uh, results on the average wage offers and wage distributions by, um, by, um, treatment. And I'm going to go in the order from the largest gap into the smallest gap. So, yeah. The previous slide, did you maybe, you said, and I missed it, I'm sorry, did you say the number, the average number of searches that people do in the voluntary experience compared to, like in the forced experience I'm seeing, more than 50% of the distribution. Is that how many times the average person is sampling? Is it like, is this something where if I said you can sample as many times as you would like conditional and seeing at least half of the distribution? Do you think this, like, would the two look more similar? So the question is, how many times do people sample in the voluntary? And the answer is, the median is 11 and the average is 14. Okay. Um, okay. So, again, I'm presenting average wage offers and the wage distributions. And on the left is going to be the wage offer bar chart. On the right is the empirical distribution. I start by presenting the baseline condition. So this is how much discrimination there is without any information. So, there is indeed a significant wage gap between the Asian and Hispanic workers. This is confirmed by the empirical distribution of the wage offers in the baseline. Next, we have experienced voluntary. We can see that experienced voluntary doesn't really meaningfully change the wage gap. And that actually tracks the pattern of beliefs. And in description, we see that, again, there's still a significant wage gap. So, description alone is not able to eliminate description. I mean, sorry, eliminate discrimination, even though, again, theoretically it has this highest information content. And now, in force 11, we see that the wage offers are already converging. Which is interesting, because it tracks the pattern of the beliefs. So, our beliefs are predicting the behavior very well here. And again, at force 20, it's also nicely converging. And here is just a summary of the wage offers in bar chart form, by frequent. So, taking stock, what can we learn about discriminatory behavior? Can you back up just a second? I'm just curious about the wage offers. Are they, they seem to be lower than beliefs, right? Yes. Yeah, so. And they're more consistent with actual ability. Sorry, what? At least for the Asian workers, they're more consistent. So, yeah, so that's a good point. So, wage offers do not necessarily need to be the same as beliefs, right? Because there's other factors that might influence it. What we're interested in here is whether there's a gap between the Spanish nation. Okay, so, we have seen that description alone cannot close the wage gap. But force experience successfully eliminated the wage discrimination. And in fact, at the relatively small sample size, 11, the wage offers are able to converge. So, what this tells us is that type of experience matters. So, because voluntary experience does not change the discriminatory patterns, there's still a very large and significant wage gap between the Hispanic and Asian groups. And by removing endogeneity and information seeking behavior, we're able to close the wage gap in the forced experience treatments. And additionally, we see that the wage discrimination patterns, they closely track belief patterns. So, that suggests to us that most of the discrimination here is actually driven by beliefs. And, um... So, Christine, if you look at the sample of people who endogenously sample 11 times, or at least 11 times, do they behave beliefs and offers, are they different from what... from those of the people who are forced to... Yes, they are. So, it's not really about the information... Yeah, it's not about the information content... Oh, sorry. But the number of people who see 11 might be a selected sample. It is. So, there are a few things going on. Yeah, so that's a good point. So, one, the people who see 11 is different from the people who are in the forced 11 treatment. Two is, those people, again, they are indulgences selecting the realization they're seeing. And they can... they can basically... when they stop at 11, it could be because they saw the stuff they liked. So... And you can test that, right? You can basically see, like, what is the stopping number? Does it correlate to their previous beliefs to get the... Yeah, so that's the next step, to analyze the sampling data. Haven't exactly figured out how to do that systematically yet. So, suggestions will be welcome. Okay. So, a brief discussion, and thank you for staying. Wait. So, the key takeaways from these results is... the... the... the... the... the... the way we present information actually does play a role in shaping beliefs and discriminatory behavior. And... by... even though... theoretically, we thought that providing complete theoretical... explicit descriptions... should directly, you know, influence beliefs and discrimination. But... what we observed in... in the... in the lab is that they cannot. And... description alone does not eliminate the belief bias, nor does it reduce discrimination. And... in the results for the experience treatments, we can see that... the experience-based learning is actually quite powerful. And... the... type of experience actually matters. So... the voluntary experience... um... does not really reduce belief bias, nor does it... uh... reduce discrimination. But... forced experience does close the belief gap and the wage gap successfully. So... what this tells us is that... um... the... endogenous information-seeking behavior in the... uh... voluntary experience... is actually... the... um... driver... of this... uh... persistent bias... and discrimination. And... removing the endogeneity in the information-seeking behavior is going to be key. And... we have shown that... the exogenous experience can successfully de-biased beliefs and... reduce discrimination. And... some policy implications, for example... for employers... if we want to reduce hiring discrimination... or... um... discriminations... uh... in evaluation against minorities... This tells us that simply... giving... for example... a report... uh... with information summarizing group performance... may not be enough. It may not be effective. But... instead... uh... the person who is evaluating workers... we can assign them to... learn about a number of workers from... the group... that may work indeed better. Because that's experience-based learning. Um... yeah... Christoph? Or... the... rationale for affirmative action. Like... when... when... when... you... get more people... from the... underrepresented group... into your institution... you see them... that's a form of experience... and then... Yeah... that's an interesting point... yeah... that's a very good point... Yeah... And... it also has... implications for... designers of... information provision experiments... So... there's a lot of... uh... information... uh... provision... uh... experience... both in the lab... in the field... intended at notching behavior... however... the result is a mixed bag... However... um... so far... like... the majority of information interventions... are... through the form of... explicit descriptions... So... what this tells us... is that... how information is presented... may be just as important... as... what kind of information is presented... So... if... uh... by designing... informations... to... incorporate... experience-based... uh... information delivery... it... might achieve... a sort of... free lunch... in terms of... improving... uh... results... uh... for behavioral nudges... Okay... so... that's... uh... uh... all I have for today... in the next steps... I have... a... a... mountain of... uh... sampling data... that I need to analyze... So... I want to... look at the... cognitive mechanisms... that lead to diverging outcomes... in... forced versus... voluntary experience... And also... I have some... open... uh... text response... questions... that I need to do... text analysis on... So... for example... I asked... participants... what do you think... the true purpose of the experiment is... um... and... how did you make decisions... for... um... the wage offers... and... beliefs... um... so... I need to analyze those... um... Again... your feedback is... very... very much appreciated... so... thank you... uh... Kaliani... Have you looked at the... correlation between the... sampling mean... for each worker... and the belief that they report... Sampling mean... So... yeah... whenever they're sampling... and that sampling mean is likely to be... different... than the population mean... which the... histogram... treatment observes... Yeah... good point... uh... I haven't looked at it... so... And... and... and... and... and... maybe that... it... it... could just be... uh... the mechanism... uh... the mechanism... one of the mechanisms... could just be that... when sampling... one by one... they... somehow are... able to... compute the mean... uh... more easily... than... computing the mean from... a... So... that's an interesting point... but... actually... I... I... I think... that... that's not the case... I don't think they're actually... computing the means... at least in the voluntary... experience... uh... Um... what I think... more likely to happen... is... Recording stops... right? Oh... no worries... what I think is probably happening is... there's probably some sort of... recency effect... in the voluntary sampling... so... they're only looking at... um... um... Um... those... most recent draws... and they're deciding to stop... based on those... and when they see... a basket of draws... that they like... they're stopping... and then... they're not incorporating... the whole... body of evidence... and... there's actually... research... with... that... so... in the... uh... literature... on the... decision... description... experience... gap... uh... people have tried... all sorts of sampling... and what they find... is that... there's... uh... stronger evidence... for the recency effect... when people... can decide... when to stop... uh... based on... uh... by observing... the... sampling... the samples... the... real-life samples... but when you force them... to... sample... a number of things... um... there's no... recency effect... so... I think... that might be... one of the mechanisms... one of the mechanisms... um... um... um... that's it... no... Fred? oh... do we have go... ol-ren... I was... you're... oh... do we have... anyone? I was... so... I think... to... like... the question... about... the people... who... voluntarily... sample... more than... eleven... times... does their behaviour look... like... have you... done... like... a quantile... ... you... do you... I... made on that... or is... like... a dummy... just... that... they're... 11... or... so... it seems like... that... that... would... like... if... if... I... sample... more... than... eleven... times... voluntarily... it... seems... like... I...I... might... be... a... very different... kind... of... person... So that's a good point. So I actually have looked at it. And actually, surprisingly, there's not a very large difference in patterns of beliefs by number of samples. This kind of further supports the idea that it's really been . The only difference is when I do a scatter plot of the beliefs by sample size, the variation in beliefs does reduce as you sample more. And the wage pattern is the same, too? Have a look at the wages. Yeah. I don't know if this is important, but in your setting, when the discriminatory decision closely tracks belief, a lot of that is coming from you incentivizing them to pay the true productivity. No, no, like I'm incentivizing them to pay their valuation. The valuation doesn't have to equal the true productivity. But isn't the lottery coming from if the sample worker's score is greater than or lower than the price list on the choice? Mm-hmm. Sorry, what? Like if, like, remember the multiple price list? Oh, yeah, yeah. Yeah. So on the left, it's hire the worker and get the score in. Yeah, and on the right is to get the fixed number, right? Yeah. Right. So you're incentivizing them to pay their belief to distribution. Right. OK. Right. So in a broader sense, the discrimination happening is not like, because I believe he has lower productivity, it's because I can't, you know? No. That doesn't have to be the case. There's different types of discrimination, right? There's discrimination driven by beliefs, which is, I believe he has lower productivity. There's beliefs driven by taste, which is, I just don't like him. And both can happen, right? And both can happen in my setting as well, because the idea of taste-based discrimination is you're willing to forego payoffs to discriminate. That's the entire basis of taste-based discrimination. So people can totally taste-based discriminate here. It's just I don't think there's a lot of evidence supporting in this environment. Yeah. You think they could be taste-based discrimination? They could, but they're not. Yeah. Because there's not actually, they know there's not another person on the other side. Right. But see, like the literature on taste-based discrimination has a very narrow definition. It has to involve interactions, which personally I don't buy. I think it doesn't have to involve interactions. You can taste-based discriminate against someone just because you don't like them. And you don't even have to know them. And that's actually what we see in the real world. That's what happened a lot right now. And this again, just maybe I'll miss the description. In the forced experience, did I see iterated one at a time, or did they see all at once? One at a time, iterated. Did you see one at a time? Sequential. So I just wanted to float an idea, and maybe the group can think about it a little bit. The stopping time issue, I think you can estimate that as a logistic regression with one observation for every draw. You mean like a time series? Yeah. And then put a one in for when they stop. And then you can look at what the cumulative mean is relative to their beliefs. That's a pretty good point. Yeah. To see if it affects their choice to stop. Yeah. That's a very good point. So you were asking for help on thinking about how to model that. I think you could probably go that route. Yeah. Very good point. Thank you. What do you cumulative mean? Like the number of. Like structured at the time series? So yeah. You go into the sampling problem. Yeah. You sample one. So now you have a sample mean with it. I choose not to stop. So that round gets a zero. Right. The next one I choose another one. My mean's updated. I don't have a mean of two. Do I go on or not? Let's say I stop. And that gets a one. And then you see the observations. And you say, OK. Are you suggesting to structure the data like crawl and stop? Yeah. Like one, three, two, four, five, zero, zero. Like you're wondering if they're going to like maybe have different patterns with Asian and Spanish. So I think the hypothesis she wants to test is whether the stopping rule depends upon the divergence of the sample from your prior beliefs. Got it. And so I mean intuitively if their sample is diverging from your prior beliefs you're going to keep drawing. And that's the hypothesis you must test. I think this is empirically would allow. I think it's more like a moving, rolling window sort of thing. It's like you're only looking at these. You could reduce the dimensionality but you're saying size is such that I don't think you need to do this. Yeah. Well the problem is like there's only about 300 something in the voluntary and then when you divide it by the Asian and Hispanic there's only 100 something. So that's the downside of this between design. It's cleaner but the power requirement is quite large. I also wanted to brainstorm about how to think about endogenous stopping. The first thing I wanted to point out is that it's not true that if they're good Bayesians they're definitely going to like draw the same number for each person. So just like one example of that. Like suppose that I'm a good Bayesian and I know for sure that like every Asian worker, like I'm 100% certain like I know exactly what's going on with Asian workers. They all get eight out of town. There's no reason for me to ever expend effort to draw any Asian. But if I have substantial uncertainty about Hispanic workers. That's the point. Okay. So you know, okay. Then that's not a very useful comment. So like how much variation there is in your priors? Yes. Right. Okay. If people held a corner solution then they wouldn't draw. Yeah. Or, but you know, it's not, you know, if they were very certain, they're very certain even if it wasn't a point mass. Okay. But to try to be more practical, like I, you know, Bayesian updating is hard. The one thing that's easy with Bayesian updating is normal, normal. So if I have a normal prior state and then I observe, and this, I'm not saying this is exactly what you're doing, but I'm trying to suggest a sort of little baby toy model that you could maybe analyze that maybe you could say, it's kind of similar to what's happening here. Right. And I analyze how it's so sort of. The nice thing is actually I have their prior distribution. Right. Okay. Yeah. And you could try to calibrate the toy model. Right. But here's the thing. Like you have a prior, maybe this is all hectic, but you have a normal prior on the state, and then your observations are going to be the true value of the parameter plus normal ones. Right. And then if, if, if, if, then you, you make an observation, so your, your prior was, was normal with mean zero and you make an observation. Mm-hm. That observation is like one, and you know that's, that's the true thing, plus like some normal noise, and you, and you know that variance that normal noise, and you have a new, your posterior is a normal with mean some combination that the formula will tell you between zero and one, and that's skinnier than, than what you, you started with. Right. You shrink them very easily. Exactly. And, and, and, okay, so, so if you, if you do this a bunch of times, then you, you just, you know, you, you start with a normal prior, you get some normal data, you have a normal posterior, and your normal posterior is your new normal prior, you do it again. It's, it's always normal. Uh, and, uh, you know, then, then, then you could, you know, try to say, well, you know, if, if you, if you have a very, uh, skinny belief for this one group, and, you, you, you know, like the, the, the, the two relevant things seem to be like, how confident you are, uh, at your prior. If you're very sure there's not a lot of reason to sample, but then also how close, um, uh, you, you know, I don't know, like some, some notion of how decision relevant to this. Because if the only decision you're going to be making is like, do I employ these guys at $10 an hour? And my initial belief is like, they're worth way less than 10. That's another reason that like, I wouldn't want to draw a lot. That's a good point. Okay. So, so maybe you could write down a model like that and then, um, Yeah, basically like improves like the point of the information is to improve belief precision. Yeah. But yeah, I have to say. Yeah, great. Thank you. Good job, Christina. Thank you, Brad. I'll revalue a couple of times. Okay, great. Thank you. And thank you, Christoph. Of course. What? Brad? It's his last name. Yeah. Just for the attendance for the. Yeah. And like, you can add him to the email list if you want. I don't know. Oh, he should be. He's not in econ. He's in finance. I think there's some, there's, I'll look at him too. I was going to, like, it seems like for the policy application, like how representative I think these groups are of, like, I might think something about like Hispanic workers as a whole versus the sample of Hispanic workers. Like here it feels like very, maybe a little bit artificial. Yeah, for sure. But you're saying like, this is representative. Yeah, for sure. 100%. This is not representative. Yeah. But of the, like, this is the group, the pool of workers. Yeah. Like you're saying like this, like the workers that you draw from actually are coming from the real sample of people. Yeah. I wonder like, if you start playing with how representative people think the people they're observing are. Could I say, well, this really shifts my belief about like this, the group that I'm seeing, but the whole, like, I wonder how much it can change beliefs about the big, like the whole big group. Like the real world. Yeah. Versus like, yeah, I think people might like justify this with like, well, if I was like, I must be observing a different subsample. Right. Yeah. Atop, you know, a positively selected subsample and keep beliefs around that way. Even if they're saying, you know, the people that are coming into an interview were somehow super positively selected, my bias shrinks for them, but something about the whole population I still maintain. Yeah, for sure. Like that's really interesting. Yeah. Like that would be totally cool if it actually works in reducing true bias, you know? Yes. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.